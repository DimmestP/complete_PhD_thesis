\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../figures/}}}

\begin{document}
\chapter{tidyqpcr: Quantitative PCR Analysis in the tidyverse}

\section{Chapter 4 Introduction}

Quantitative PCR is the most common technique for the quantification of DNA and RNA. 
The specificity and sensitively of the assay has led it to be considered the gold standard for nucleotide detection and quantification in medicine, legislation and academia \parencite{Kubista2006}.
However, qPCR experiments, especially assays requiring a reverse transcriptase step, are susceptible to several reliability issues if poorly designed \parencite{Bustin2002}.
Nevertheless, the widespread use of qPCR across distinct disciplines has enabled the creation of countless protocols, equipment and analysis methods without consistent standards \parencite{Bustin2021}.
The limitations and varying quality of qPCR experiments has lead to a reproducibility crisis with significant consequences for academia \parencite{Garson2009} and public health \parencite{Bustin2013}. 

qPCR assays are susceptible to multiple biases that can cause highly variable or unrepeatable results \parencite{Bustin2002}.
Lab specific protocols for extraction, pipetting and storage can introduce variation, with samples having $R^2 = 0.4$ between extraction methods \parencite{Bustin2017, Dagnall2017}.
A sample's position on a qPCR plate can also have a significant contribution to measured expression with one study showing $10\%$ of the variation between replicates was due to well position \parencite{Eisenberg2015}. 
Positional effects can be introduced as edge wells may be more susceptible to evaporation or thermal gradients may be uneven across the plate. 
Finally, qPCR experiments that include a reverse transcriptase step can determine Cq values that differ by up to 91-fold according to the choice of reverse transcriptase and sensitivity to original RNA concentration \parencite{Stahlberg2004}.
As different reverse transcriptases interact with RNA secondary structures in different ways, the effect is not consistent across targets \parencite{Brooks1995, Williams1992}.

In an effort to improve the reproducibility of qPCR results guidelines were created for publishing qPCR data called the Minimum Information for the Publication of Quantitative Real-Time PCR Experiments (MIQE) guidelines \parencite{Bustin2009}.
They provide a thorough checklist of every detail that needs to be reported in order to enable another researcher to accurately repeat an experiment. 
Over time they developed their own file format, the Real-Time PCR Data Markup Language or RDML, to standardise the way scientists describe their experiments which users can upload to an open source database \parencite{Lefever2009, Ruijter2015}.
The guidelines have been around for over a decade, but they are still not widely implemented with as little as $4\%$ of qPCR articles citing them \parencite{AbdelNour2020}.

In order to improve the reproducibility of qPCR analyses, we propose the development of a novel analysis package that uses detailed documentation and open software practices to teach and facilitate transparent and quality controlled qPCR analysis. 
In addition, the dependence on proprietary software for the calculation of threshold Cq values and quality control graphs remains a stumbling block for reproducible and trustworthy analysis. 
We believe a novel package developed in the programming language R, a language regularly used by biologists, could increase the quality of qPCR assays from initial experimental design to the final publication of results.
The consistent structure of qPCR data also provides an opportunity to apply scalable data analysis practices.
Cq values are the variables of interest for most qPCR experiments and they are inherently separated into values from independent wells. 
Therefore, developing functions that can take advantage of this regular, independent structure to Cq values can lead to comprehensive and efficient analysis of data sets of varying sizes.

Data analysis practices have been developed to enable efficient and scalable analysis by modularising analysis pipelines and standardising the structure of data sets.
Google's MapReduce and Hadley Wickham's split-apply-combine focus on separating the analysis into independent functions and applying each function to subsets of the full data set before combining the results to produce a final summary \parencite{Wickham2011,Dean2004}.
Modularising the analysis follows the DRY philosophy as a task, such as calculating the mean and standard deviation, can be encapsulated in one function but applied to different subsets of the data.
Designing analysis programs in a modular way also simplifies the task of parallelising the process, which leads to a scalable method as large data sets can be subsetted into manageable chunks according to the computation resources available at run-time \parencite{Chua2004}.

To supplement the application of modular analysis practices, there is a broader idea of structuring data in a tidy format to ensure no information is lost when it is split into subsets \parencite{Wickham2014}. 
Data is considered tidy if it follows a strict row and column structure. 
Each column must hold all values of a variable and each row holds a separate observation.
Unfortunately, data in the wild is often organised with combinations of observations and variables being spread across columns and rows.
However, ensuring each row contains all the information from an observation simplifies the grouping and splitting of a data set.
If all functions in an analysis pipeline accept and output data in a tidy form then the entire analysis can be applied to different groupings of data leading to a more comprehensive analysis.

The split-apply-combine paradigm developed by Hadley Wickham has been incorporated into the R programming language through the tidyverse.
The tidyverse suite of data analysis packages contains functions covering tasks from graph plotting to conducting statistical tests which all follow a tidy architecture \parencite{Wickham2014}. 
The implementation of the split-apply-combine paradigm within the tidyverse is facilitated by the \lstinline{group_by} function.
Passing a variable held in the column of a tidy data set to the \lstinline{group_by} function splits the data into groups of observations with the same value in that column.
All functions within the tidyverse are then applied to each group separately and their results are combined. 
In addition, the functions inside the tidyverse are templates for user friendly software development.
Their source code is accessible and comprehensible as they follow a strict coding style and contain extensive documentation.
The functions are also intuitive to learn as they follow a verb-object naming convention that states what they do and what they act on.


Here, we outline the development of tidyqpcr, an R package built within the tidyverse which implements the MIQE guidelines to facilitate scalable, transparent and quality controlled qPCR analysis.
We begin by outlining the continued need for better design and analysis of qPCR data through a review of current software.
tidyqpcr is then introduced as an R package that helps users create publication ready figures of normalised qPCR results. 
tidyqpcr packages the principles outlined in the MIQE guidelines for the easy design, analysis and reporting of reproducible and accurate qPCR results.
Well documented functions and intuitive structure help users conduct reproducible research without depending on checklists.
The chapter ends by discussing improvements to the package motivated by a series of user tests and code reviews as well as future extensions to its functionality.

\section{Chapter 4 Results}

\subsection{qPCR analysis software review}

We review the current software landscape for analysing qPCR data and outline the continued need for scalable, user friendly, and MIQE compliant analysis software.
In the last two years, the use of qPCR in global response to the COVID-19 pandemic has driven the development of new packages to support the reliable detection of COVID-19.
However, the last published review of qPCR analysis packages was \cite{Pabinger2014}.
The review covered 27 different open source packages with the R package qpcR highlighted as the tool with the most comprehensive functionality.
It also described the varying quality of documentation, lack of compliance with the MIQE guidelines, inconsistent input and output file formats, and the use of CLIs over GUIs across the reviewed packages.
A review of qPCR analysis software released since 2014 is described below in order to A) determine the need for another qPCR analysis package and B) discern any generalisable changes in qPCR analysis software since the last review.

A list of qPCR analysis software was gathered through searches on GitHub, bioconda, bioconductor, CRAN, and Google Scholar.
All of the software packages reviewed are freely available for use and are open source.
The majority of the packages were released after the previous published review.
However, HTqPCR, qpcR, ReadqPCR, and NormqPCR are included for completeness as they are dependencies for several of the newer packages.
The packages are grouped according to their primary usage: Web Apps require a server and typically provide a website for users to conduct analyses, R and Python packages primarily need to be downloaded and ran locally, and Misc requires other proprietary software. A table summarising the main functionality of all reviewed software can be found in Appendix A.

\subsubsection{Software descriptions}

\textbf{Web Apps}

\textbf{QuantGenius} A PHP web-app published in Feb 2017 for the quantification of target abundance using standard curves. 
Users manually copy Cq values for each target into the GUI and can export results as a comma separated file or excel file. 
The app automatically highlights samples that are outliers, are outside the limit of detection or have poor efficiency, but does not check melt/amplification curves. 
It does not contain any functions for conducting statistical analyses or for producing graphs. 
QuantGenius has not been updated since publication \parencite{Baebler2017}.

\textbf{ELIMU-MDx} A PHP web-app published in Oct 2019 for the storage and analysis of clinical qPCR data. 
The app extracts Cq values from the input RDML file and stores results as an RDML file. 
The PHP backend is able to deduce relative and absolute abundance as well as detect samples that are outliers, are outside the limit of detection or have poor efficiency. 
Users need to set up their own apache or nginx server to run analyses and store the database. 
It does not contain any functions for conducting statistical analyses or for producing graphs. ELIMU-MDx was last updated in Dec 2020 \parencite{Krahenbuhl2019}.

\textbf{PIPE-T} An extension to the Galaxy web-based bioinformatics platform published in Oct 2019 for the relative quantification of qPCR data. 
It accepts Cq values for each sample/replicate/condition as separate tab separated files and outputs tab separated files. 
The extension facilitates a variety of Cq normalisation methods, mainly provide through the R package HTqPCR. 
QC can be conducted by flagging samples with Cq levels outside user defined thresholds, but no melt/amplification curves are available. 
There are also functions to test for significant differential expression in two condition experiments and to impute missing data. 
PIPE-T has not been updated since publication \parencite{Zanardi2019}.

\textbf{SATqPCR} A standalone web-app published in Aug 2019 for the relative quantification of qPCR data. 
It accepts up to two tab separated text files as input: one contains a table of Cq values and primer efficiencies with columns representing different genes and rows representing samples, the other optional file relates samples to different factors for t-test or anova statistical tests. 
It outputs summary statistics and normalised Cq values in text files and as bar charts in PNG format. 
The software cannot calculate primer efficiencies but, if efficiencies are provided by the user, it can use primer efficiencies in the relative quantification calculation. 
It does not have any functionality to plot melt/amplification curves, detect outliers or interpolate missing data. 
The app does contain an algorithm to automatically detect the most stable genes and use them as normalising genes. 
The app is an update to a previous R package called RqPCRAnalysis, but has not been updated itself since publication. \parencite{Rancurel2019}.


\textbf{Python packages}

\textbf{Auto-qPCR} A standalone web-app with Python back-end published in Oct 2021 for the relative and absolute quantification of qPCR data. 
It accepts a comma separated file or text input file with specific columns names, such as well, sample name, target name, and Cq value. 
It outputs text files and bar charts in PNG format with normalised $\Delta$Cq, $\Delta\Delta$Cq or absolute copy number results. 
Users can download the python code to run the app locally or use the online server. 
The function for calculating relative Cq values does not include primer efficiency. 
The software does not process melt or amplification curves but uses a standard deviation cutoff for outlier identification. 
It also can conduct a 1 or 2 way anova to test for significance. Auto-qPCR has not been updated since publication \parencite{Maussion2021}.

\textbf{qpcr}
A python package released in Aug 2021 for the relative quantification of qPCR data. 
It accepts comma separated files and excel files in a variety of different formats to import different combinations of experiments, targets and samples. 
It outputs text files and bar charts in JPEG format with normalised $\Delta\Delta$Cq values. The software does not process melt or amplification curves but uses a standard deviation cutoff for outlier identification. 
It does not contain any functions for conducting statistical tests but can calculate primer efficiencies and use them in the $\Delta\Delta$Cq calculations. 
qpcr was last updated in Feb 2022 \parencite{Kleinschmidt2022}.

\textbf{R Packages}

\textbf{Chainy} An R Shiny web-based app published in May 2017 for the relative quantification of qPCR data. 
It accepts inputs in multiple forms including RDML files and several proprietary qPCR analysis software output files. 
It outputs a zip file of summary statistics and normalised Cq values in comma separated files and as bar charts in PNG format. 
The software can calculate Cq values and efficiencies directly from amplification curves using the qpcR package or accepts pre-determined values. 
It can determine stable normalising genes using the NormqPCR package and flags outlying samples that do not fit the sigmoidal amplification curve. 
The app can also determine significant fold changes between samples using a permutation test. 
Chainy was last updated in Aug 2020 \parencite{Mallona2017}.

\textbf{shinyCurves} An R Shiny web-based app published in Oct 2021 for detecting viral infections from diagnostic qPCR assays. 
The app accepts excel spreadsheet and RDML file inputs from BioRad's proprietary analysis software CFX Maestro Software, Roche's LightCycler® Software, Agilent's Aria software and Applied Biosystems® qPCR analysis software.
It outputs the results as comma separated files. 
The plate designs are either 96 or 384 wells and users can flag control wells if they follow specific formats. 
It extracts Cq values from excel spreadsheets and determines if samples are Positive, Negative or Undetermined for viral load depending on user defined thresholds. 
It can quantify abundance using a standard curve if the input files contain serial dilutions. 
Users can conduct QC by viewing melt and amplification curve plots created by the R package qpcR and define a standard deviation cutoff for outlier identification.  
It does not contain any functions for conducting statistical analyses. shinyCurves has not been updated since release \parencite{OlaecheaLazaro2021}.

\textbf{LEMming} An R script published in Sept 2015 for the relative quantification of qPCR data. 
It proposes a linear error model for qPCR experiments which it uses to normalise Cq values without the use of normalising genes. 
This novel normalising method can confound the treatment effects with some systematic errors. 
Therefore, if normalising genes have been verified, the standard $\Delta$Cq method is recommend. 
It does not provide any methods to read in qPCR data and creates an R S4 class object as output. 
It does not conduct any standard QC checks such as plotting amplification curves or checking for outliers.  
It does include how to conduct several different differential expression tests. LEMming has not been updated since release \parencite{Feuer2015}.

\textbf{pcr} An R package published in May 2018 for the relative quantification of transcript abundance. 
It does not provide any methods to read in qPCR data but expected the input to be structured with each row a different sample and each column a target gene. 
The package creates an R data.table of summary statistics and ggplot2 figures. 
If 100\% primer efficiency is assumed, it can calculate $\Delta$Cq. 
Otherwise, it requires serial dilutions to create standard curves and deduce relative abundance. 
The package also includes functions to conduct t-tests, Wilcoxon signed-rank tests  and ANOVA.
However, it always normalises to one normalising gene. It does not conduct any standard QC checks such as plotting amplification curves or checking for outliers. 
However, if the assay includes serial dilutions then amplification efficiency can be checked before analysis. 
pcr was last updated in April 2020 \parencite{Ahmed2018}.

\textbf{HTqPCR} An R Bioconductor package published in Dec 2009 for the relative quantification of qPCR data. 
It contains several functions to read in several proprietary qPCR analysis software files. 
It outputs normalised Cq values and summary statistics as an S4 class object as well as several plots. 
The software does not plot melt or amplification curves as QC, but does allow users to define a standard deviation cutoff for outlier identification. 
There are also functions to determine batch effects, spatial effects and hierarchical interactions across samples and experiments. 
It can normalise genes using the standard $\Delta$Cq method or, in the case of unreliable normalising genes, it can normalise by quantile means and rank-invariant normalising factors. 
The package also contains functions to test differential expression with linear models, Mann-Whitney test or t-tests. 
HTqPCR core functionality has not been changed in 10 years, but it is maintained by the R Bioconductor community \parencite{Dvinge2009}.

\textbf{ReadqPCR and NormqPCR} A pair of R Bioconductor packages published in July 2012 for the relative quantification of qPCR data. 
ReadqPCR contains functions for reading in raw Cq value files from several several proprietary qPCR analysis software files. 
They output normalised Cq values and summary statistics as S4 class objects as well as several plots. 
The software does not plot melt or amplification curves, but uses a user defined standard deviation cutoff for outlier identification. 
NormqPCR can select reliable normalising genes and impute missing values. 
It does not contain any methods for detecting statistically significant differential expression. 
ReadqPCR and NormqPCR were last updated in July 2018 \parencite{Perkins2012}.

\textbf{qpcR} An R package released in 2008 for selecting the best sigmoidal model to fit to the amplification curve of each qPCR target for the accurate determination of Cq values and PCR efficiency. 
It does not provide any methods to read in qPCR data, but outputs an S3 object with summary statistics. 
The package contains several methods to determine the model with the best fit which is then used to determine threshold Cq values and efficiency. 
It can detect sample outliers, calculate relative and absolute abundances, and plot summary data. qpcR was last updated in June 2018 \parencite{Ritz2008}.

\textbf{Misc}

\textbf{Spreadsheet} A guide published in Dec 2021 for standardising the use of spreadsheet software to determined relative abundance. 
It does not describe how to calculate primer efficiencies but does use them in the $\Delta$Cq calculations. 
It outlines the use of t-tests to determine statistical significant differences.  
The guide does not process melt or amplification curves but suggests using a standard deviation cutoff for outlier identification. 
It does not suggest how to plot any summary statistics \parencite{Ng2021}.

\subsubsection{Summary}

Novel qPCR analysis packages continue to be released despite packages being available across platforms for decades and the fundamental principles of qPCR remaining unchanged. 
Similar to the conclusions of the Pabinger \textit{et al} review, the packages have a wide range of functionality, documentation and compliance with the MIQE guidelines. 
Two reoccurring issues with the reviewed software are the lack of quality control checks and the inconsistent approaches to removing data points that are considered outliers.
Over the last 8 years there has been an increase in GUI based apps, but most sacrifice scalability and reproducibility to maximise ease of use. 
The R package qpcR remains the most comprehensive analysis package for qPCR data. 
However, the depth of its functionality is limited by its documentation.
The package includes a description for every function and its arguments, but it does not provide enough information for deciding between alternative options or to justify the arguments it sets as default.
The package also does not provide an example workflow to show how its functions can be combined to complete an analysis.
In addition, the software architecture does not follow the tidy paradigm.

\subsection{tidyqpcr: Quantitative PCR Analysis in the tidyverse}

tidyqpcr addresses the need for a qPCR analysis package that balances functionality, documentation, and quality control to facilitate reproducible and best-practice compliant analysis.
It is intended to be flexible enough to analyse qPCR data from any nucleic acid source - DNA for qPCR or ChIP-qPCR, RNA for RT-qPCR - on any scale - 96, 384, 1536+ well plates.
Currently tidyqpcr has functions that support relative quantification by the $\Delta Cq$ method, but not yet absolute quantification.
A key component of tidyqpcr is its comprehensive documentation that teaches users how to use tidyqpcr and explains tidyqpcr’s design decisions.
These openly accessible teaching materials helps to improve an entire qPCR experiment, from plate design to publication ready figures.
The package follows the FAIR principles - Findable, Accessible, Interoperable, and Reusable - to ensure every stage of the analysis is transparent and verifiable. 
tidyqpcr is available to use now and can be downloaded from our GitHub page, \href{https://github.com/ropensci/tidyqpcr/}{github.com/ropensci/tidyqpcr/}.


\subsubsection{tidyqpcr design principles}


\textbf{Flexible and scalable analysis} Within the R programming language, the tidyverse suite of packages have pioneered the use of tidy analysis. 
Mimicking the tidy structure in the creation of tidyqpcr not only opens the way to flexible analysis enabled by simply following the tidy data paradigm, it also directly allows access to a plethora of open-access and scalable data analysis tools already created in the tidyverse, Figure \ref{fig:tidyverse-ecosystem}. 
Once users familiarise themselves with the tidy paradigm, they can conduct advanced downstream analysis such as linear regression analysis, complex visualisation and statistical summaries. 

\begin{figure}[t]

{\centering \includegraphics[width=0.8\linewidth]{tidyverse_ecosystem} 

}

\caption[Developing tidyqpcr using the tidyverse packages grants access to a larger ecosystem of data analysis packages.]{\textbf{Developing tidyqpcr using the tidyverse packages grants access to a larger ecosystem of data analysis packages.} The triangle contains the core tidyverse packages used to develop tidyqpcr. The wider ecosystem of packages all follow the same split-apply-combine paradigm and can be applied to any data analysed by tidyqpcr enabling statistical analysis or enhanced reproducibilty.}\label{fig:tidyverse-ecosystem}
\end{figure}

\textbf{Experimental design}  In tidyqpcr, we help experimentalists decide how to set up their experiment by providing several plate plan helper functions built around block designs.
This enables samples to be spread across the plate and minimise well position biases but still contain regular patterns for loading with multi-channel pipettes, Figure \ref{fig:combined-plate-design}.
We also describe in detail different plate design strategies that users can explore depending on their pipettes and plates.
Users can exclude loading samples into edge wells with the provided helper functions.
We are also exploring introducing automatic generation of loading recipes for common liquid handlers so users with the access to the appropriate equipment can ensure the loader and plate plan match identically.
Grouping biological/technical replicates so they are placed in the neighbouring wells can lead to systematic biases in the results.
In an ideal situation, different samples and their replicates should be allocated entirely random well positions.
However, if the sample loading is manual, then having inconsistent plans across plates will complicate the loading process and increase the likelihood of a mistake.
Ultimately, having an incorrect map of samples in wells is significantly more detrimental to any analysis than systematic bias.
tidyqpcr provides balance between easy loading and good experimental design principles.

\begin{figure}[p]

{\centering \includegraphics[width=1\linewidth]{combined_plate_plans.png} 

}

\caption[tidyqpcr facilitates flexible, modular plate design.]{\textbf{tidyqpcr facilitates flexible, modular plate design.} \textbf{(A)} Single repeatable block containing important well information: Sample\_id, target\_id, RT and bio\_rep number. 
\textbf{(B)} Alternative plate design with alternate sample replicates. Useful for pipetting with multi-channel pipettes. 
\textbf{(C)} Full 96 well plate design based on the repeatable block of panel \textbf{A}. 
Shows a MIQE-compliant plate with 3 technical replicates, 2 biological replicates and -RT controls.} \label{fig:combined-plate-design}
\end{figure}


\textbf{MIQE-compliant results} tidyqpcr follows the MIQE-guidelines for analysis by allowing multiple normalising genes by default in the delta-Cq calculation. 
Helper functions are also provided for the design of serial dilutions plates for primer calibration are available together with functions to calculate linearity, $R^2$ and plots to display behaviour across multiple primers.
Importing the fluorescence across all cycles is also available so that quality control graphs for the melt and amplification curves across all wells can be seen, Figure \ref{fig:plate-amp-curves}.
There are also default functions to plot Cq values across the plate to see biases, such as edge well bias. 
The vignettes also outline a reproducible analysis pipeline to standardise the analysis so reviewers can check it.
All vignettes also describe the use of technical replicates, biological replicates and wells that contain RNA samples without reverse transcriptase.

\begin{figure}[t]

{\centering \includegraphics[width=1\linewidth]{example_melt_and_curve_plots.png} 

}

\caption[Extensive vignettes teach users MIQE-compliant analysis.]{\textbf{Extensive vignettes teach users MIQE-compliant analysis.} \textbf{(A)} Amplification curve plots verifying that curves of the control samples without reverse transcriptase (dotted lines) do not overlap with the samples of interest.
\textbf{(B)} Melt curves showing double stranded DNA fragments become single stranded above the threshold temperature for samples of interest.} \label{fig:plate-amp-curves}
\end{figure}

\subsubsection{Functionality}

\textbf{Overview}
tidyqpcr provides the functionality to aid with the implementation of qPCR assays from design to analysis. 
The design of complex plate plans is facilitated with the use of general plate formatting and labelling functions such as \lstinline{label_plate_rowcol} and \lstinline{create_blank_plate}. 
Meanwhile, helper functions that create commonly used 96 and 384 well plates are provided: \lstinline{create_colkey_4diln_2ctrl_in_24} and \lstinline{create_rowkey_4_in_16}. 
Once the plate has been designed, users can import the completed qPCR assay data from proprietary qPCR software. 
There are default functions for imported Roche LightCycler® data \lstinline{read_lightcycler_1colour_raw} but users can create functions to import data from other machines as long as the end data frame is in a tidy format.
The import functions can be used to import threshold Cq values calculated from the qPCR  machine or the Cq values across cycles for the entire time course.
There are functions to conduct quality control on the imported data, as required by the MIQE guidelines. 
\lstinline{calculate_drdt_plate} calculates the derivative of the melt curve enabling the user to confirm amplification occurs only at one temperature.
Plate effects such as reducing efficiency at edge wells can be inspected using \lstinline{display_plate_value}.
Before the experiment of interest can be conducted, the amplification efficiency of the primers must be tested in order to ensure the assumptions of the qPCR threshold Cq comparisons are valid.
Users can calculate primer efficiency with \lstinline{calculate_efficiency_bytargetid} which enables calculations across targets given appropriate dilution assay data.
Finally, on the completion of the quality control steps user can calculate $\Delta$Cq and $\Delta\Delta$Cq values from any combination of samples and targets using \lstinline{calculate_deltacq_bysampleid} and \lstinline{calculate_}\lstinline{deltadeltacq_bytargetid}. 
The mechanics around calculating $\Delta$Cq across replicates and plates is enabled by the \lstinline{group_by} function in the tidyverse applied on the key variables \lstinline{sample_id} and \lstinline{target_id}.
We chose the name \lstinline{target_id} to hold information about the target of the qPCR primers and \lstinline{sample_id} to hold the condition/strain/biorep information from each sample.
Both words are necessary in the plate data frame in order to use tidyqpcr functions. 
The decision to name them \lstinline{sample_id} and \lstinline{target_id} was a balance between being specific enough to avoid ambiguity, but general enough to enable a variety of qPCR assays to be incorporated.  

\textbf{Use Case}

An example use case is now described to show the power of tidyqpcr to analyse a complex 96 well qPCR assay.
The data set is an assay inspecting the change in expression of 16 gene associated with the yeast stress response, as provided by Dr Edward Wallace.
Yeast samples are exposed to heat shock in the presence of transcriptional inhibitors.
The two transcriptional inhibitors are Phenanthroline and Thiolutin.
Therefore, there are six conditions: no inhibitor present with and without heat shock, Phenanthroline with and without heat shock, and Thiolutin with and without heat shock.
There are three technical replicates, two experimental replicates and each sample has a control which has not had any reverse transcriptase added.
The example code will design the plate for this experiment, read in the results of the Roche LightCycler® qPCR machine and calculate normalised Cq values for all target-condition combinations.

The first stage of conducting a qPCR experiment with tidyqpcr involves the designing a plate with the \lstinline{label_plate_rowcol} function, Listing  \ref{tab:create-plate-plan}. 
It requires three data frame arguments: a blank plate data frame holding the shape and number of wells to be used, a rowkey data frame holding row-wise experimental metadata and a colkey data frame holding column-wise experimental metadata.
The blank plate data frame can be any custom shape or size and follow any labelling system as long as each column and row is uniquely identifiable. 
tidyqpcr does provide boilerplate 96, 384 and 1024 well plates.
The tidyqpcr example vignettes encourage users to hold \lstinline{target_id} data in the rowkey data frame and \lstinline{sample_id} data in the colkey data frame.
This leads to entire rows containing the same primer and technical replicates and controls being grouped together.
Creating the rowkey in the example is straightforward as there are the same number of rows as \lstinline{target_id}s so the mapping is one to one.
In other cases, if the number of \lstinline{target_id}s is a factor of the number of rows then the \lstinline{target_id}s are replicated until all rows are filled.
This pattern is an easy way of introducing biological replicates onto a plate. 
Designing the colkey can be more complicated as different combinations of conditions, replicates and controls need to be included.
Similar to the rowkey, if the number of unique samples is a factor of the number of columns then they can be repeated in blocks to represent the technical replicates and -rt control.
Although it is not strictly necessary to use \lstinline{label_plate_rowcol} to create a plate, we designed the function to encourage the users to design the plate in a logical row-wise and column-wise manner.
This leads to a intuitive and reproducible method to load the plate which minimises mistakes and increases efficiency. 

\makeatletter
\renewcommand{\fnum@table}{Listing \thetable}
\makeatother

\begin{table}
\centering
\begin{tabular}{ |p{5.6cm}  p{5.6cm}|}
\hline
  \begin{lstlisting}[style=mystyle]
# list target_ids of primer sets
target_id_levels <- c("HOR7",
   "HSP12", "HSP26", "HSP78",
   "HSP104", "RTC3", "SSA4",
   "PGK1", "ALG9", "HHT2",
   "HTB2", "RPS3", "RPS13",
   "RPS15", "RPS30A", "RPL39")

# Set up experimental samples
heat_levels <- c("-", "+")
heat_values <- factor(
   rep(heat_levels, each = 3),
   levels = heat_levels)
drug_levels <- c("C", "P", "T")
drug_values <- factor(
   rep(drug_levels, times = 2),
   levels = drug_levels)
condition_levels <- paste0(
   drug_levels,
   rep(heat_levels, each = 3))
\end{lstlisting}
 & 
 \begin{lstlisting}[firstnumber=20,style=mystyle]
condition_values <- factor(
   condition_levels,
levels = condition_levels)
 
# create plate plan
rowkey <- tibble(
   well_row = LETTERS[1:16],
   target_id = factor(target_id_levels, 
    levels = target_id_levels))

colkey <- create_colkey_6_in_24(
   heat = heat_values,
   drug = drug_values,
   condition = condition_values)

plateplan <- label_plate_rowcol(
   create_blank_plate(
      well_row = LETTERS[1:16],
      well_col = 1:24),
   rowkey, colkey)
\end{lstlisting} \\ 
\hline
\end{tabular}
\caption{Example tidyqpcr code for designing a 96 well qPCR plate for an assay with 16 target stress response genes across 6 conditions.}
\label{tab:create-plate-plan}
\end{table}
After the qPCR experiment has been conducted, the next step is to read in the results, Listing \ref{read-raw-data}. 
The function \lstinline{read_lightcycler_1colour_cq} is the default function in tidyqpcr for reading in the calculated threshold Cq values held in the excel file format used by the Roche LightCycler® software. 
The complementary function \lstinline{read_lightcycler}\lstinline{_1colour_raw} enables user to load the Cq values across the entire time course for plotting quality control figures. 
Users using qPCR machines other than a Roche LightCycler® currently need to create their own function for reading in Cq data. 
The plate plans defined above can then quickly match the Cq values with the sample metadata. It is vital that the row and column labelling used by the qPCR machine is repeated correctly in the plate design data frame. 
As can be seen in the example code, the scalability of tidyverse functions enables tidyqpcr to easy incorporate multiple experimental replicates without significant changes in the pipeline.
\begin{table}
\centering
\begin{tabular}{|p{5.6cm}  p{5.6cm}|}
\hline
 \begin{lstlisting}[firstnumber=40, style=mystyle]
 file_path_cq_plate1 <- 
    system.file("extdata",
      "Edward_qPCR_TxnInhibitors_
         HS_2018-06-15_
         plate1_Cq.txt.gz",
      package = "tidyqpcr")

plate1 <- file_path_cq_plate1 %>%
read_lightcycler_1colour_cq() %>%
   left_join(plateplan,
      by = "well") %>%
   mutate(biol_rep = "1",
      plate = "1")

file_path_cq_plate2 <-
   system.file("extdata",
   "Edward_qPCR_TxnInhibitors_
      HS_2018-06-15_
      plate2_Cq.txt.gz",
   package = "tidyqpcr")
\end{lstlisting} &
\begin{lstlisting}[firstnumber=60, style=mystyle]
plate2 <- file_path_cq_plate2 %>%
read_lightcycler_1colour_cq() %>%
  left_join(plateplan,
     by = "well") %>%
  mutate(biol_rep = "2",
     plate = "2")

# combine data from both plates into a single data frame
plates <- bind_rows(plate1,
   plate2) %>%
  unite(sample_id, condition,
     biol_rep, sep = "",
     remove = FALSE)
\end{lstlisting} \\
\hline
\end{tabular}
\caption{Example tidyqpcr code for reading threshold Cq values from LightCycler® qPCR machines and combining them with the designed plate plan.}
\label{read-raw-data}
\end{table}

Finally, to complete this example analysis the function \lstinline{calculate_deltacq_}\lstinline{bysampleid} will normalise all the Cq values from the targets of interest to the normalising genes, Listing \ref{calc-delta-cq}. 
Following the MIQE guidelines,  this function can accept multiple \lstinline{target_id}s as normalising genes and calculate a mean or median value to subtract from all targets of interest. 
Again, using the flexibility of the tidyverse the mean Cq across any combination of samples, replicates and experiments can be calculated. 
This is possible because tidyqpcr consistently follows the tidy paradigm across all function outputs. 
The comparison of expression across all conditions and targets is plotted using ggplot2, Figure \ref{fig:tidyqpcr-multi-plate}.

\begin{table}
\centering
\begin{tabular}{| p{5.6cm}  p{5.6cm} |}
\hline
 \begin{lstlisting}[firstnumber=70, style=mystyle]
 platesnorm <- plates %>%
  filter(prep_type == "+RT") %>%
  calculate_deltacq_bysampleid(
     ref_target_ids = "PGK1")

platesmed <- platesnorm %>%
  group_by(sample_id, condition, biol_rep, heat, drug, target_id) %>%
  summarize(
    delta_cq = median(delta_cq,
       na.rm = TRUE),
    rel_abund = median(rel_abund,
       na.rm = TRUE))
 \end{lstlisting}& 
 \begin{lstlisting}[firstnumber=83, style=mystyle]
ggplot(data = platesmed) +
  geom_point(aes(x = target_id,
     y = rel_abund, 
     shape = biol_rep,
     colour = drug),
    position = position_jitter(
       width = 0.2,
       height = 0)) +
  facet_wrap(~heat, ncol = 3) +
  scale_y_log10("mRNA relative detection",
     labels = scales::label_number()) +
  theme(axis.text.x = 
     element_text(angle = 90,
        vjust = 0.5))
\end{lstlisting}   \\
\hline
\end{tabular}
\caption{Example tidyqpcr code for for calculating $\Delta$Cq across multiple plates and plotting summary results across target genes.}
\label{calc-delta-cq}
\end{table}

\begin{figure}[t]

{\centering \includegraphics[width=\linewidth]{example_normalised_deltacq_multiplate_figure}}

\caption[tidyqpcr can be used to quickly analyse multi-plate, multi-target, and multi-sample qPCR assays.]{\textbf{tidyqpcr can be used to quickly analyse multi-plate, multi-target, and multi-sample qPCR assays. }}\label{fig:tidyqpcr-multi-plate}
\end{figure}

\textbf{Function definitions and Documentation}

tidyqpcr functions are designed following the tidyverse guidelines for compatible functions, Listing \ref{define-function}. 
The verb-object naming convention is followed throughout to help ensure each function is clearly named according to its purpose. 
The first argument of any tidyqpcr function is the primary data frame to be acted on. 
This allows the pipe operators commonly used in tidyverse code to continue their primary function. 
The input data frame is expected to be in the long tidy format and the outputs of any tidyqpcr function is also a data frame in long tidy format. 
The definition of the \lstinline{calculate_deltacq_bysampleid} function has been copied below as an example of a typical function code. 
The function groups Cq values by \lstinline{sample_id} and subtracts the normalising \lstinline{target_id} values from all Cq value in each group. 
Therefore, Cq, \lstinline{sample_id}, and \lstinline{target_id} are vital variables and are checked to be in the supplied data frame before the function attempts to calculate $\Delta$Cq. 
Once the presence of the required variables is asserted, the function calculates $\Delta$Cq and adds it as a new variable to the data frame. 
The function is entirely scalable as the internal \lstinline{group_by} function can handle any number of \lstinline{sample_id} and the $\Delta$Cq's can be calculated by any number of normalising \lstinline{target_id}s.



\begin{table}
\centering
\begin{tabular}{| p{5.6cm}  p{6.1cm} |}
\hline
\begin{lstlisting}[style=mystyle]
calculate_deltacq_bysampleid <- 
 function(cq_df,
          ref_target_ids,
          norm_function = median) 
{

 assertthat::assert_that(
  assertthat::has_name(
   cq_df, 
   c("target_id",
     "sample_id",
     "cq")))
     
 cq_df %>%
  dplyr::group_by(
   .data$sample_id) %>%
\end{lstlisting} &

\begin{lstlisting}[firstnumber=17, style=mystyle]
   dplyr::do(
    calculate_normvalue(
     .data,
     ref_ids = ref_target_ids,
     value_name = "cq",
     id_name = "target_id",
     norm_function = 
      norm_function)) %>%
  dplyr::rename(
   ref_cq = 
    .data$value_to_norm_by) %>%
      dplyr::ungroup() %>%
      dplyr::mutate(
       delta_cq = 
        .data$cq - .data$ref_cq,
       rel_abund = 
        2^ -.data$delta_cq)}
\end{lstlisting} \\
\hline
\end{tabular}
\caption{Function definition for the calculate $\Delta$Cq method within tidyqpcr. This example showcases the use of the \lstinline{group_by} function provided by the core tidyverse package dplyr to split the Cq values by \lstinline{sample_id} and apply the \lstinline{calculate_normvalue} on each group separately.}
\label{define-function}
\end{table}
Preceding the function definition is several commented lines documenting the details and use cases of the function, Listing \ref{function-documentation}. 
First, a brief description of the function, its input arguments and expected output is provided. 
Then, its dependencies on other functions both inside tidyqpcr and in other R packages are listed. 
Finally, short examples showing the use of the function are outlined. 
This preamble is converted into markdown formatted help documentation by the R package roxygen2 \parencite{Wickham2021}. 
This documentation is accessible using the base help command once tidyqpcr has been downloaded and as a standalone documentation website hosted by rOpenSci.
\begin{table}
\centering
\begin{tabular}{| p{6.1cm}  p{5.6cm} |}
\hline
\begin{lstlisting}[style=mystyle]
#' Calculate delta cq to normalize 
#, quantification cycle (log2-fold)
#' data within sample_id.
#'
#' This function implements 
#' relative quantification by the 
#' delta Cq method. For each 
#' sample, the Cq values of all 
#' targets (e.g. genes, probes, 
#' primer sets) are compared to 
#' one or more reference target 
#' ids specified in 
#' `ref_target_ids`.
#'
#' @param cq_df a data frame 
#'  containing columns `sample_id`,
#'  value_name (default `cq`) and
#'  tid_name (default `target_id`).
#'  Crucially, sample_id should be
#'  the same for different technical
#'  replicates measuring identical 
#'  reactions in different wells of 
#'  the plate, but differ for 
#'  different biological and 
#'  experimental replicates. See 
#'  tidyqpcr vignettes for examples.
#' @param ref_target_ids names of 
#'  targets to normalize by, i.e. 
#'  reference genes, hydrolysis 
#'  probes, or primer sets. This can 
#'  be one reference target id,
#'  a selection of multiple target
#'  ids, or even all measured 
#'  target ids. In the case of all 
#'  of them, the delta Cq value 
#'  would be calculated relative to
#'   the median (or other 
#'  `norm_function`) of all measured 
#'  targets.
#' @param norm_function Function to 
#'  use to calculate the value to
#'  normalize by on given scale. 
#'  Default is median, alternatively 
#'  could use mean.
#'
#' @return data frame like cq_df 
#'  with three additional columns:
#'   ref_cq,  cq value for reference 
#'            target ids;
#'   delta_cq,  normalized value;
#'   rel_abund, normalized ratio.
\end{lstlisting} &
\begin{lstlisting}[firstnumber=52, style=mystyle]
#' @export
#' @importFrom tidyr %>%
#' @importFrom stats median
#' @importFrom rlang .data
#' @examples
#' # create simple cq dataset 
#' # with two samples, two 
#' # targets  and 3 reps
#'
#' cq_tibble <- tibble(
#'  sample_id = rep(
#'   c("S_1", "S_1", "S_1", 
#'     "S_2", "S_2", "S_2"),
#'     2),
#'  target_id = rep(
#'   c("T_1",
#'     "T_norm"),
#'   each = 6),
#'  tech_rep = rep(1:3, 4),
#'  well_row = rep(
#'   c("A", "B"),
#'   each = 6),
#'  well_col = rep(1:6, 2),
#'  well = paste0(well_row,
#'                well_col),
#'  cq = c(10, 10, 10, 12,
#'         12, 11,  9,  9, 
#'          9,  9,  9,  9))
#'                      
#' # calculate deltacq using
#' # reference target_id 
#' # called 'T_norm'
#' 
#' # use case 1: 
#' # median reference 
#' # target_id value
#'
#' cq_tibble %>%
#'  calculate_deltacq
#'       _bysampleid(
#'    ref_target_ids = "T_norm")
#' 
#' # use case 2: 
#' # mean reference target_id 
#' # value 
#'
#' cq_tibble %>%
#'  calculate_deltacq
#'           _bysampleid(
#'   ref_target_ids = "T_norm",
#'   norm_function = mean)
\end{lstlisting} \\
\hline
\end{tabular}
\caption{Function documentation for the calculate $\Delta$Cq method within tidyqpcr as structured by the roxygen2 R package.}
\label{function-documentation}
\end{table}
\newpage
\textbf{Tests}

tidyqpcr follows software development best practices by incorporating unit tests for all vital functions within the package, Listing \ref{function-tests}. 
95\% of all functions within tidyqpcr are covered by a test. 
The development of tidyqpcr uses the continuous integration available in GitHub as a GitHub Action runs each unit test to check for bugs with every commit to the repository. 
The tests consist of small use cases with the simplest expected outcome from each function being compared to the actual output. 
Functions with multiple possible behaviours according to optional arguments have multiple tests to ensure functions perform as expected. 

\begin{table}
\centering
\begin{tabular}{| m{10cm} |}
\hline
\begin{lstlisting}[style=mystyle]
test_that("Unit test for the calculate_deltacq function",
{
   simulated_48_well_plate_plan <- create_blank_plate_96well() %>%
      dplyr::filter(well_row %in% c("A", "B",
                                    "C", "D")) %>%
      dplyr::mutate(
         target_id = rep(c("Target_1", "Target_2",
                           "Target_3", "Target_4"),
                         each = 12),
         sample_id = rep(rep(c("Sample_1", "Sample_2",
                               "Sample_3"),
                             each = 4),
                         times = 4),
         tech_rep = rep(c(1, 2, 3, 1),
                        times = 12),
         prep_type = rep(c("+RT", "+RT",
                           "+RT", "-RT"),
                         times = 12))

   calculated_48_well_plate_with_deltacq <- 
      calculate_deltacq_bysampleid(
         simulated_48_well_plate_with_cq %>%
            dplyr::filter(prep_type == "+RT"), 
         ref_target_ids = "Target_3") %>%
            dplyr::arrange(well_row, well_col)

    expect_equal(calculated_48_well_plate_with_deltacq,        
                 simulated_48_well_plate_with_deltacq)})
\end{lstlisting} \\
\hline
\end{tabular}
\caption{Function test for the calculate $\Delta$Cq method within tidyqpcr as structure by the testthat R package.}
\label{function-tests}
\end{table}

\subsection{Reviewing and improving tidyqpcr}

\subsubsection{Themes derived from semi-structured interviews}

We conducted a series of semi-structured interviews to explore current practices in qPCR experimental design and analysis.
These interviews were conducted over zoom with the video recorded by zoom's proprietary software over a period between 45 and 90 minutes.
The interview explored whether users were aware of the MIQE guidelines and if they currently executed any QC measures. 
We also wanted to know about the typical experiments users conducted; i.e. qPCR machine, reagents, and plate plans. 
Finally, we wanted to explore what software users currently used to analyse their results and if they were interested in learning R based analysis. 
The questions guiding the interview are available in Appendix A.
The six interviewees covered several academic roles from senior post-doctoral research assistants to undergraduates.
The interviewees had varied experiences in programming based analysis and in conducting qPCR assays.
The transcripts from the interviews are available at \href{https://doi.org/10.5281/zenodo.7101606}{doi:10.5281/zenodo.7101606}.
Once the interview was completed the interviewers were introduced to tidyqpcr and asked to complete a user test.

Several key themes relating to the design and analysis of qPCR experiments appeared across the interviews, Figure \ref{fig:semi-structured-test-cloud}.
In terms of the design of qPCR assays all interviewees reported doing RNA not DNA quantification.
The inclusion of three technical replicates was typical although some users had a process of removing outliers.
It also was not common practice to check amplification curves or confirm linear efficiency.
Overall, few were aware of the MIQE guidelines and few recall published data giving QC results, analysis code or detailed protocols.
In addition, no-one reported trying to recreate any other published data set and a regular theme of not trusting conclusions based on qPCR results alone was common.
In terms of the analysis of qPCR data Excel remains a common piece of software for the analysis and design of plates.
Users almost universally depend on proprietary qPCR analysis software to determine Cq values.
Few were aware of the concept of 'tidy' data outside of users already using R packages  based on the tidyverse.
Although most users are confident they could re-analysis their own results no-one reported that their analysis was openly available for reviewers to access.

\begin{figure}[t]

{\centering \includegraphics[width=0.5\linewidth]{mg_rb_ck_ec_db_semi_structured_word_cloud} 

}

\caption[A text cloud showing the key words repeatedly used across the semi-structured interviews.]{\textbf{A text cloud showing the key words repeatedly used across the semi-structured interviews.} The greater the frequency of a word the larger it appears in the figure. }\label{fig:semi-structured-test-cloud}
\end{figure}

\subsubsection{rOpenSci review and JOSS submission}
In order to ensure tidyqpcr followed best software development practices and to verify the reliability of its functions, we submitted the package for an rOpenSci code review. 
rOpenSci offers transparent, constructive and open reviews of R packages that lower barriers to working with local and remote scientific data sources.
A successful rOpenSci review can then be submitted to the Journal of Open Source Software (JOSS), enabling the software development work to be officially acknowledged with a citation.
The submission to JOSS was publish in June 2022, \href{https://joss.theoj.org/papers/10.21105/joss.04507}{doi:10.21105/joss.04507}.

The rOpenSci review highlighted several issues with tidyqpcr's implementation, including: insufficient compliance with FAIR software practices, failing CRAN software repository checks, and the inclusion of redundant functionality. 
rOpenSci requires all submitted software to be Findable, Accessible, Interoperable and Reusable (FAIR) and follow the practices of the CodeMeta Project (\url{https://codemeta.github.io/}).
rOpenSci required us to add an codemeta.json file which standardises how the metadata associated with the software is held, such as maintainer details or software dependencies.
Standardising the format of the metadata ensures a) all the data required to reuse a piece of software is available, and b) search functions can be developed to find software based on different metadata tags.
Acceptance in to rOpenSci suite of peer reviewed packages also includes submission to the R package repository CRAN.
However, the example data sets and extensive vignettes included in the tidyqpcr package led to it being over 5Mb, which is too large to be hosted on CRAN.
We managed to reduce the package size by converting the example data set files to compress zip files.
It was also highlighted that some functions originally packaged within tidyqpcr to aid with producing interpretable graph labels appeared out of place.
We removed these plot helper functions as the scales package also provided the functionality \parencite{Wickham2022}.

\subsubsection{User Feedback}
Following the Agile software development practice, feedback from users was used to influence development priorities at regular intervals. 
Feedback was acquired from multiple sources: user testing following the user interviews outlined above, an independent code review by a bioinformatics research assistant and from the rOpenSci code review.
The task-based user test section of the interview focused on three main themes of tidyqpcr: block based plate planning, tidyverse based API and conducting reproducible analysis.
An independent code review following the Google code review guidelines was conducted by a colleague who had not previously contributed to the development of tidyqpcr.
An overview of major improvements to tidyqpcr's functionality, usability, and documentation in response to user feedback is available in Table \ref{user-feedback}.


\makeatletter
\renewcommand{\fnum@table}{Table \thetable}
\makeatother

\begin{table}
    \def\arraystretch{1.25}
\centering
\begin{tabular}{|| m{4.5cm} | m{7cm} ||} 
 \hline
 \textbf{\large Issue} & \textbf{\large Solution} \\ [0.5ex] 
 \hline\hline
 \multicolumn{2}{|l|}{\textbf{Functionality}} \\
 \hline
 tidyqpcr contains helper functions to create 96 and 384 well plates but 1536 plate wells are not supported. & 
 Created a helper function to automatically create 1536 plate as well as a function to produce a "pick list" based on the plate to facilitate the use of robotic sample loaders. \\ 
 \hline
 Quality control should include a method for checking for positional effects on a qPCR plate. & 
 Created the display\_plate\_value function to visualise threshold Cq values across the plate following the user defined plate plan. \\
 \hline
 \multicolumn{2}{|l|}{\textbf{Usability}} \\
 \hline
 Determining general but intuitive names for function arguments. & 
 Depending on assay used the measurement variable could be called Primer Set (for SYBR dye-style) or a fluorescent-quenched probe. 
 Rather than committing to a specific assay we decided on the more general term target\_id. \\
 \hline
 The benefit of using tidyqpcr over the other available packages remains unclear. & 
 The GitHub Repo README file now contains a summary table comparing functionality and MIQE compliance across tidyqpcr and its popular alternative packages.\\
 \hline
 \multicolumn{2}{|l|}{\textbf{Documentation}} \\
 \hline
 Current package vignettes overwhelm new users as they introduce the basic concepts of tidyqpcr on multi-condition, multi-target data sets. & 
 Interviewee provided a simpler 96-well plate data set for us to use as an example. 
 We created a simpler vignette introducing the basic concepts of tidyqpcr using this data set for users to understand before moving onto the larger example. \\
 \hline
\end{tabular}
\caption[tidyqpcr's development included improvements  from user feedback.]{The tidyqpcr development cycle included regular opportunities for users to suggest improvements.}
\label{user-feedback}
\end{table}

\subsubsection{Future functionality}

tidyqpcr is a fully functional package for the analysis of qPCR data using SYBR Green assays from Roche LightCycler® qPCR machines. 
However, there remains several planned improvements to enable tidyqpcr to easily analyse data from other qPCR assays and major additions to complete its aim to analyse qPCR data in an entirely open way according to the MIQE guidelines.
First, to extend the import functions to read formats from other qPCR machines we intend to incorporate the plater R package \parencite{Hughes2016}.
This package follows the same tidy data principles as tidyqpcr, but is built to read data formats from a variety of plate based experimental assays.
Next, tidyqpcr has only been tested to work on SYBR Green qPCR assays.
Fluorescent-quenched probe based qPCR assays allow multiplexing so each well can measure multiple targets. 
Acquiring suitable fluorescent-quenched probe data and adding documentation on how tidyqpcr can be used to analysis such assays would also lead to addition functionality. 

The previous improvements enhance the use cases of tidyqpcr, but tidyqpcr needs additional functionality to fulfill its aims to be MIQE compliant and entirely open source.
The major additions are: to include primer efficiency into $\Delta$Cq calculations, to provide methods to determine appropriate normalising genes, to import and export qPCR metadata in RDML form, and to calculate threshold Cq values itself. 
geNorm is an established method for incorporating primer efficiency into Cq values 
Meanwhile, the R package NormqPCR already contains the functionality to determine appropriate normalising genes to determine $\Delta$Cq values from a group of candidate genes. 
Integrating the geNorm method and NormqPCR functionality within the \lstinline{calculate_normvalue} is a priority.
As the default file type for MIQE compliant qPCR analysis, creating the functionality to update RDML files using the RDML R package would increase the ease of conducing MIQE compliant qPCR assays \parencite{Roediger2017}.
Finally, tidyqpcr needs to remove its reliance on the threshold Cq value calculations conducted by proprietary software. 
There are multiple methods to determine threshold Cq values with most being available within the qpcR R package described above.
Rewriting the the comprehensive set of functions in qpcR to follow a tidy format usable by tidyqpcr should complete the open source requirement from tidyqpcr's statement of intent. 

\section{Chapter 4 Conclusion}

qPCR remains one of the most widely used microbiology assays with uses across medicine, law and biology. However, awareness of what is required for someone else to repeat an experiment remains a key obstacle in ensuring reliable, reproducible results. 
Solutions have been widely published, such as the MIQE guidelines, but few publications appear to follow them. 
In addition, surveying the current qPCR analysis software landscape it is clear that there remains a gap in accessible, reproducible and scalable analysis packages. 
A combination of low levels of awareness and high levels of required effort impedes the publication of protocols and analyses in an open way. 
The consequences of the lack of reproducibility continues to have a detrimental effect on the quality and reliability of conclusions based on qPCR data. 
In summary, there remains a demand for methods that support and teach the best practices in qPCR assay design and analysis.


Here, we have described the R package tidyqpcr which aims to facilitate the analysis of qPCR data in an open, reproducible and reliable way. 
Created using best practices in open source software development, the tidyverse suite of data analysis packages and the MIQE guidelines we believe this tool can help experimentalists improve the quality of their analysis and the confidence in their results.
We utilised multiple documentation formats to enable users to access the required level of detail for their expertise. 
Step-by-step workflows are provided in detail in the vignettes with specific reference to the MIQE guidelines and experimental plans that include controls, replicates, and block designs. 
More experienced programmers can take advantage of the complete function documentation to cherry pick the steps they require and combine with the plethora of tidyverse packages to create bespoke pipelines of their own.
We believe that the extensive work put in to ensuring the package is an open and accessible will manifest in to users contributing their own improvements and functions to the package. 
The initial response from users outside of our lab have been positive suggesting this package can improve the quality and clarity of results across the wider research community.

\end{document}
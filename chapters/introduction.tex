\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../figures/}}}

\begin{document}

\chapter{{Introduction}}

The acquisition of quality, high-throughput data sets is outpacing the creation and implementation of methods to handle and analyse it. 
In 2012, the European Bioinformatics Institute (EBI) was one of the biggest biology repositories in the world with a 20Pb storage facility \parencite{EBI2012}. 
However, by 2021, the upload of new data reached 20Pb a year with the institute having to explore collaborations with Google and Amazon in order to keep up \parencite{EBI2021}. 
The “data deluge” \parencite{Royal2012} is not only affecting infrastructure but is leading to fundamental questions about the philosophy of science as scientific publications are starting to benefit from the quality and quantity of their data rather than the significance of the conclusions they reach \parencite{Leonelli2019, Botstein2010}. 
The pursuit of big data in biology is undoubtedly leading to answers to questions thought unassailable a few years ago, such as AlphaFold base-pair resolution protein structures for entire genomes \parencite{Jumper2021}. 

Significant progress has been made in developing methods that select relevant features in the sparse high-dimensional data sets.
In investigations of linear covariates, robustness to noise and outliers can be improved by using alternative loss functions, such as the least absolute deviation, or the introduction of penalising terms, such as the $L_n$-norm of the regression coefficients \parencite{Wu2015}.
Reducing the dimensionality of data sets to emphasise regions of interest has also become standard through methods such as principle component analysis \parencite{Wall2005}.  
Methods to comparing alternative models of high-dimensional data have been explored including error estimation with cross-validation \parencite{Molinaro2005} or calculating the Akaike information criterion (AIC) \parencite{Akaike1998}.
Finally, conservative methods to test the significance of multiple features whilst maintaining power are widely implemented, e.g. the false discovery rate (FDR) \parencite{Benjamini1995}.

An alternative approach to analysing high-dimensional data sets is the application of flexible deep-learning algorithms.
A variety of ML architectures have been successfully applied to big data across biology ranging from detecting cancer to predicting gene expression \parencite{Xie2017, Liang2015, Tang2019}.
However, complex models appear to be less tolerant of experimental noise and more susceptible to confounding batch effects than simpler models \parencite{Cortes2015, Kaiser2019, Schmitt2021}.
The decisions made by deep-learning algorithms also tend to be difficult to interpret limiting their contribution to improving our understanding \parencite{Caruana2015, Ribeiro2016}.
Other probabilistic methods, such as Bayesian hierarchical models, can offer a balance between interpretability and flexibility, which will be discussed later.
However, whatever the inherent structure of the algorithm its effectiveness will be decided by the implementation by the end user. 
Unfortunately, big data scales up the issues inherent to modern biological research alongside novel solutions. 

Science is suffering from a massive reproducibility crisis. 
In 2017, Nature surveyed over 1500 scientists and found over 70\% of them tried and failed to reproduce someone else's work \parencite{Baker2016}.
The origins of the crisis come from the lack of detail in experimental protocols and misunderstanding of statistical tests.
The level at which scientists mis-used statistical tests has even led on journal to ban any reference to statistical significance \parencite{Trafimow2015}.
Fundamentally, the work by computational biologists is just as susceptible to the reproducibility crisis \parencite{Garijo2013}.
How data is preprocessed, how missing values are dealt with and which software is used can all drastically change results \parencite{Ioannidis2009}.
As the volume and types of data increases, the sparsity of biologically relevant data points increases which only decrease the likelihood of an analysis being reproducible \parencite{Chattopadhyay2019}.

There remains a gap between the computation methods used to analysis high-dimensional data and the implementation of these methods on biological data.
Those that bridge this gap are typically biologist with no formal software engineering training \parencite{Attwood2019} and little motivation to develop prototype analysis scripts into fully fledged programs \parencite{Prins2015}. 
In addition, selection of appropriate models are dependent on subject, hypothesis and quality of data \parencite{Ching2018}.
This leads to high duplication, poor reproducibility and slower overall progress \parencite{Lawlor2015}.
Only through the use of rigorous statistical methods implemented in reproducible and usable software packages can the dawn of the big data effectively combat the reproducibility crisis in biology.

Here, I discuss work using data sets from high-throughput transcriptomics experiments to expand the quantification of regulatory effects on mRNA transcripts. 
Through the development of analysis software and statistical models that incorporate both experimental noise and non-standard interactions I explore transcript localisation and the context dependence of cis-regulatory elements. 
The introduction starts by covering the basics of key transcriptomic assays that have enabled the dawn of the 'data deluge' in molecular biology. 
qPCR, microarrays and RNA-Seq are covered with emphasis on the sources of error that can be present in these experiments. 
Then, I discuss a handful of statistical methods that enable rigorous analysis of the multi-dimensional data sets with the emphasis of discovering sparse features with biological significance. 
Finally, I outline the key software development practices that have influence the work I have conducted. 
I implement these best practices in open source software development to ensure the analysis software I develop is accessible, interoperable and usable.

\section{High-Throughput RNA Assays}

\subsection{qPCR}

Polymerase chain reaction (PCR) is regarded as one of the most significant methods in molecular biology with its inventor Kary B. Mullis being co-awarded the Nobel prize in Chemistry in 1993.
The accuracy of PCR as a method to produce copies of regions of DNA in a log-linear manner has led researchers to explore its use an accurate method to quantify abundance since its invention in the 1980's, \parencite{Saiki1988}.
The introduction of fluorescent probes into the PCR  \parencite{Holland1991}, but it wasn't until the 2000's \cite{Walker2002}



%What is LOD and amp curve?

The basic principle of a polymerase chain reaction consists of the duplication of a region on DNA that is specified by two short nucleotide sequences, called primers, that are designed to be complementary to the start and the end of the region of interest. 
A highly thermal tolerant polymerase, adapted from the bacteria \textit{Thermus aquaticus}, is then able to complete the duplication of the region by elongation of the sequence between the two primers \parencite{Holland1991}.
The duplication cycle is repeated several times leading to the number of copies of the original region grows exponentially.
The PCR polymerase must be thermal tolerant as the duplication cycle is rapidly repeated by raising the temperature to melt the newly created complementary stand away from the original strand before dropping the temperature back down to enable the next round of elongation. 

If this cycle is repeated in a solution with surplus concentrations of primers, enzymes, and nucleotides the increase in copies per cycle is bound by an exponential defined only by the number of transcripts of the target sequence in the original sample.
Therefore, if the exponential growth of PCR duplicates can be measured and accurately modelled then the original number of transcripts can be inferred. 
To measure the exponential duplication, Quantitative PCR introduces dyes that only fluoresce when a region has been successfully duplicated. 
The fluorescence of the sample is measured as the PCR cycle is repeated to determine the exponential growth in duplicates and quantify the number of transcripts of the target sequence in the original sample.

\subsubsection{qPCR Methods: RNA vs DNA}

qPCR is highly optimised for amplifiying DNA fragments using the \textit{Thermus aquaticus} polymerase. Therefore, to quantify RNA fragments an additional step creating cDNA from RNA using a reverse transcriptase. Unfortunately, this step can be a significant source of variation and is regularly determined to be the source of most variation between RNA samples \parencite{Stahlberg2004}. The reverse transcriptase priming method, original RNA target concentration and total RNA concentration in the sample can all vary the cDNA yield between replicates. In order for a RT-qPCR experiment to be reproducible the reverse transcriptase step must be properly optimised and clearly described. 

\subsubsection{qPCR Methods: Taqman vs SYBR Green}

# figure outlining different methods


fluorescence resonance energy transfer (FRET) probe

The fluorescent dye used to detect duplicates differs significantly between the two most common qPCR methods: Taqman and SYBR Green. SYBR Green methods contain dye that fluoresce when they bind to any double stranded DNA species in the sample. This leads it to being a cheap and relatively easy to use system, but it is highly susceptible to contamination and it is unable to distinguish between duplicates from different species. TaqMan methods bind the fluorescent dye to a oligonucleotide probe that is designed to attach to the region of interest somewhere between the two primers. In addition to the fluorescent dye on one end the oligonucleotide probe has a quencher on the other. The quencher stops any emissions from the dye from being detected. However, during elongation, when the polymerase reaches the oligonucleotide probe, the probe is hydrolysed separating the dye from the quencher. Emissions from the fluorescent dye can then be measured and the creation of a new duplicate is detected. The introduction of a custom oligonucleotide probe increases the specificity of TaqMan methods and reduces the effects of contamination. Also, the abundance of multiple species can be measured in the same sample by carefully designing different oligonucleotide probes with different fluorescent dyes. Unfortunately, the design and creation of custom probes causes TaqMan methods to be more expensive and technical. Meanwhile, the accuracy of TaqMan methods is comparable to the cheaper SYBR Green methods, if correctly conducted. \cite{Tajadini2014}

\subsubsection{Quantifying Abundance: Curve Fitting vs Cycle Threshold}

The exponential limit of the duplicate number enables two methods that summarise amplification curves to compare abundance across samples. Assuming all samples reach the exponential growth stage in copy number per cycle at the same time then the difference it fluorescence at any cycle of the PCR assay dependent only on the original copy number. Therefore, if you set a particular fluorescence threshold during the exponential phase to compare all samples at, the number of cycles needed to reach that fluorescence level is an accurate proxy for original copy number. A common method to determine target abundance is the number of cycles required to reach a set fluorscence. Unfortunately this method assumes both that each sample reaches the exponential phase at the same time and that each cycle doubles the number of duplicates perfectly for each sample. An alternative method fits an sigmodal curve to the amplification curve and uses this model to deduce the cycles required to reach a the threshold. The additional fitting can account for difference in the times to reach the exponential growth phase between samples and can directly account for deviations in perfect duplication. 

\subsubsection{Quantifying Abundance: Relative vs Absolute}

Multiple methods exist for converting fluorescence measurements into quantitative values for sample abundance whilst accounting for experimental and technical noise. First, qPCR experiments can be designed either to measure the relative change in abundance across samples or estimate the absolute number of copies of a target in a sample. Relative abundance measurements depend on the determination of genes that have constant expression across all samples/conditions. Any change in the gene(s) of interest across samples can then be detected by comparing expressions relative to the set of constantly expressed genes. Normalising the fluorescence to genes with constant expression minimises batch effects introduced by sample preparation, reverse transcription and reactants. Absolute quantification of a target requires a preliminary experiment where known initial quantities of the gene of interest are measured with qPCR. The amplification curve for each sample, with gradually increasing copies of the gene of interest, is measured to create a collection of standard curves. Next, the sample from the primary experiment is measured with qPCR and its amplification curve is compared to the standard curves. The absolute copy number of the gene of interest in the experimental sample can be interpolated.

\subsection{Micro-arrays}

\subsection{RNA-seq}
RNA-Seq analysis consists of three core steps: Quality Control, alignment and counting. The exact quality control steps can change significantly between types of assay. For example, enrichment of mRNA transcripts using degradation or poly(A) anchors need to be checked for effectiveness by inspecting ribosomal RNA content or tRNA levels. Meanwhile in the case of single-cell RNA-Seq checking for cases where two or more cells may have accidentally been combined (as doublets are common in occurrence in many technique) is a vital step that is not required for bulk RNA-seq methods. However, across all methods it will be required to check for sequence amplification biases, calling quality and whether each lane has successfully detect reads. Once QC has been completed it is then important to remove any adapters and UMI that may have been introduced during the library preparation as these may complicate alignment to the organism genome as they will not be included. Also it is common to trim the 3' end of reads as more error in nucleotide callings tend to occur at then end. If the assay also includes multiplexed samples these need to detect and separated. Once the reads have been trimmed and multiplexed, then they need to be aligned to the organism genome in order to be able to determine which gene they correlate to. A variety of genome aligners are available depending on organism and computing infrastructure limitations.  BowTie2 is regularly held as the most accurate aligner but it is not splice aware. Other aligner like star or hi-sat2 are much better at aligning reads across splicing junctions. There is a significant difference in speed and memory requirements between the two and users need to decide what is the priority for them. It is vital that quality control steps are conducted after the alignment step. Are the vast majority of reads aligned to your organism genome or could a contaimenant be present. Visualing reads on a genome browers is important at this point too to check you have correctly defined strandedness. Once the sequences have been aligned the next step is to remove PCR duplicates if UMIs are present. If the UMIs with random sequences are present in reads aligned to the exactly the same gene then they are considered duplicates and can be flatten to just one read. Finally, with the deduplicated aligned reads fully processed the user can then count the number of reads to each gene or conduct other downstream analyses such as detecting isoforms.

Reproducible analysis of RNA-seq data is crucial as software versions, computer systems and servers can all contribute to different outcomes. Using workflow management programs, such as NextFlow, also enable scaleability (analysis of results on a computer to using a university server or cloud service). The analysis steps will be the same but the number of core/input files/ repeats of steps will change mostly without the need of user input. Combining workflow languages with containers: singularity, docker, ect, or environments: conda, renv, ect, and enabling your analysis scripts to be publicly available simultaneously allows you to conduct the same (or similar) analysis in the future and encourages reviewers to actualy confirm you analysis is correct and that there are no issue with QC.

\section{Statistical methods}

\subsection{Linear Regression}

penalised linear regression

\subsection{Gaussian Processes}
Gaussian processes are a Bayesian method developed for analysing time series data. Time series data is structured differently to other continuous data types as it often has strong autocorrelation between neighbouring points and repeating cycles. Standard linear regression models often struggle to model this behaviour (although circular patterns can be learnt from finite linear combination of sin/cos functions). They are the projection of Gaussian distribution into infinite dimensions. By defining the covariance function, which relates the value of each point to every other point, can a various attributes to the fitted model. For example, if the fitted line must be continuous and differentiable then the covariance functions must be similarly defined. Similar to a finite multi-dimensional Gaussian it is fully defined by two parameters:  the mean function and the covariance function. As it is a Bayesian method it also create an error at in the expected value at any point. The covariance function defines how smooth the function mapping neighboring points are. You can use a number of well characterised functions as the covariance function or you can use a neural network to learn complex connections. 

The omniplate python package developed by the Swain lab uses a Gaussian process with an infinitely differentiable covariance functions. The enables the time series protein florescence data to be model, then doubly differentiated to find the point of max increase in florescence. It is important to use a similar time point to compare all the different constructs at and the time of max growth rate is appropriate. Partly because it is simple to calculate whilst making sure all the constructs are being compared at a similar point in the growth cycle (mid exponential). It also important to account for auto-fluorescence and the size of cells in various media in order to remove confounding contributions to sample fluorescence.

\subsection{Bayesian Statistical Models}

To be able to compare samples across RNA-seq runs users need to be able to remove biases that changes from run to run. For example, the amplificaition of reads from each lane of a RNA-seq machine varies significantly. Therefore direct comparison of transcript counts mapped to a genome will, for the most part, depend on the total reads read by that lane of the RNA-seq machine.  Other amplification biases can be introduced, the shotgun method of short read sequencing means the number of reads per gene will be proportional to the length of that gene. Simply because longer genes can be separated into more fragments. In addition elongation biases between the nucleotides can change the amplificiation ratio of genes. Certein gene with high GC content may not be amplified as well because they bind too tightly for the melting stage to occur efficiently. 

Raw reads are rarely used as the measure of expression between samples and between genes. Instead, they are normalised either by internal methods or by external controls such as spike-in samples. Internal normalisation commonly consists of converting raw reads into transcripts per million or reads per million. It can been widely reported at the read per million method has significant biases and should not be used. However, transcripts per million account for the total read variation between runs and the gene length biases. However, dividing by total reads does leave TPM counts vunerable to being dominated by the behaviour of a few highly expressed genes. Since there is a large order of magnitude between expression across an organisms genome, gene that are expressed on the order of $10^4$ transcripts per cell will contribute more to the total reads compared to transcript that only occur once or twice. If the experiments significant change the expression of the highly expressing genes but the users is mostly interested in the behavour of the lowly expressing genes, comparing TPM may confound expression patterns. Neither method attempts to deal with GC biases. Alternatively, an external control of known volumes of synthetic RNA can be introduced with differing GC content and lengths. The comparison of these RNA levels after being amplified in the RNA-seq assay can discover biases in transcript length, total read and GC content. A rigorous method for using this information to normal counts across the genome has not be developed as error in spike-in volumes are very common and obscure predictions expected reads post-sequencing.

\section{Research Software Engineering}

The don't repeat yourself (DRY) paradigm is one of the most commonly implemented practices to encourage programmers to write shorter and specific functions to solve regularly occurring tasks. Minimising the number of repetitions helps reduce errors (simply from imperfect copying), improves readability and enables faster debugging as compartmentalising tasks into separate functions enables you to test each function separately.

% user feedback and interviews
Software developers are often unaware of their own unconscious knowledge which may create a barrier for users to engage with user. 
Also, language differences between disciplines and non-native speakers can create obstacles.

Creating function tests is an example for when the DRY principle is counterproductive.
As a correctly design suite of test are intended to help catch bugs during the development stage repeating how input arguments are defined within each test help speed the diagnosis process.
Abstracting error messages and test to the shortest, most general form can invalid their usefulness in diagnosing bugs.

%DRY paradigm  https://en.wikipedia.org/wiki/The_Pragmatic_Programmer

% what is research software https://f1000research.com/articles/8-1353#ref-16

The development of high-throughput, multi-omic experiments across the biological sciences has led to an unprecedented demand on software for research. In the late 90's less than 20\% of research papers mentioned the use of software in their research. In 2021, over 70\% of publications stored on pubmed cited the use of software. Furthermore, articles accepted in the highest impact journals cited more pieces of software on average. (sentence emphasising use of statistical software). Software developed specifically to answer research questions has rapidly become a cornerstone of the modern empirical method. \cite{Schindler2022}

Despite its importance, research software development remains a nascent subdomain of software development. The academic position of research software engineer was only coined in the late 2000's \cite{Prause2010} with the creation of the society of software engineers being founded in 2010. Exemplary research groups and institutions exist with the primary aim of developing software to answer, or facilitate others in answering, research questions (SSI etc). However, it still remains a domain primarily occupied by self-taught programmers as an aside from their primary domain of expertise.  The software itself is regularly developed with specific short term goals during the latter stages of scientific inquiry to provide statistical summaries and deduce significant discoveries. Therefore, practitioners are rarely trained in the standard development practices that are used across all other professional software development contexts and they often consider the analysis steps a final hurdle before completing a project/publication.

This leads to common patterns that define research software. Research software is typically written in high level scripting languages (R/Python), often to produce statistical summaries of complex datasets as a means to gather evidence to support a conclusion in the latter stages of scientific inquiry (typically. during the steps of writing a publication for submission).

\subsection{Software development practices}

As the practice of software development became a profession, practitioners have created frameworks to facilitate efficient and rigorous software development.
The first software development framework, and the one still most used in the sciences, is called Waterfall.
Waterfall introduces a structure to the coding practice by outlining a series of stages that are completed linearly.
It starts with a detailed specification before moving to development and finally implementation.
Alternative practices generally deviate from the waterfall ethos of leaving implementation to the very last stage.
Instead they prioritise creating a minimal viable product as soon as possible and testing its functionality.
Others even enable the specification to be altered and redefined as the project develops.
The flexibility to redefine the project as it is being developed is why the alternative practices are called agile. 

Two common agile practices in industry are scrum and extreme programming.
Scrum is the modern archetypal agile method.
Instead of fixing the development schedule as each stage much be completed sequentially the project is broken into sprints each iteratively adding some functionality which is reviewed tested and implemented before moving onto the next.
Constantly reviewing and testing the code enables programmers to catch bugs early (ensuring complex dependency issues don't arise) receive feedback on whether initial functionality is actually useful and achievable.
As its name suggests extreme programming pushes the principles of agile programming to the extreme
Updates to the software are expected on a weekly basis with additions tested and implemented. 
In addition programmers are expected to conduct paired programming. 
Two programmes work together at all time with only one coding while the other verbally dictates what should be added. 
Although this reduces the number of lines of code written per developer, with two coders constantly reviewing each other code the number of programming bugs introduced is reduced which is assumed to negate any reduction in productivity.

Does research software differ from general software? (in terms of aims, requirements and expectations)

Software development methodologies from Waterfall to Extreme focus entirely on efficient and accurate frameworks for writing code. 
Although these methodologies differ in the timing and extent of feedback from primary stakeholders none explicitly state time for writing documentation. 
Software is only as effective as its user.
Coding practices are highly developed. Efficient coding practices, interactions between teams and automatic integration and testing of work. In practice, software is limited by the users understanding of its implementation.
Documentation practices are less developed.

% https://scrumguides.org/docs/scrumguide/v2020/2020-Scrum-Guide-US.pdf#zoom=100

% https://hygger.io/blog/extreme-programming-waterfall-agile-kanban-scrum-lean/#extreme-programming-vs-waterfall

% https://ieeexplore.ieee.org/abstract/document/1231158?casa_token=6sbQr1xrIy4AAAAA:0GmEQG8RPMLhd5563BpXhyfQqSPd7Tw88hrhpX2uo4bu3LgewtZNrNKTYggK0JMFJpBvsFMUS2c

% https://peerj.com/articles/cs-835/#results--analysis-of-software-mentions

\subsubsection{Computation and the environment}

A fast growing aspect of software development practices is the contribution of compute to the emission of CO2.
It is estimated that the IT sector will be responsible for 20\% of the world's CO2 production by 2030. 
Incorporating carbon footprint estimates into coding specifications and benchmarking published code may be the norm in the future
Balancing efficiencies and speed for new software updates may need to be encouraged.
Depending on highly inefficient older code may be detrimental to the carbon footprint but the latest version my prioritise speed over memory/CPU usage and actually increase emissions.
In addition, programming philosophies may need to be developed. 
Time tends to be the limiting factor in many analyses so swapping to faster code is nearly always the priority. 
However, is there a limit to how much we prioritise speed? 
If each run emits more CO2 and we conduct far more runs than was initially thought necessary will that always be seen as the ideal.

Two common sequence aligners hi-sat 2 and star are noted for their significant difference in memory consumption.
Previously research has noted that memory usage is overtaking CPU as the leading energy consumer in severs.
Over allocating memory to a task can significantly increase energy usage with 90\% of energy spent on maintaining memory in the background regardless of load.
Interestingly, the consequences of agile development practises with weekly or even daily testing could contribute to significant increases CO2 emissions.
Tools such of GitHub actions do enable quick detection of bugs but does the significant increase in compute worth the cost?

% https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009324

% https://dl.acm.org/doi/pdf/10.1145/3076113.3076117?download=true

\subsection{Open source software development}

The growing expectation for a science that has its results open to all to read, data freely available to use and decisions that are completely transparent leads to an natural proclivity towards the already developed world of open software. The software development practices described above can be followed with a closed development process or an open one. It is fairly prolific across research to leave any code used in a research paper to be 'available upon request'. However, the encouragement of open research software practices offers many of the advantages currently seen in industry. The success of the Google supported Machine Learning framework TensorFlow showcases the enhanced re-usability, collaboration and accessibility enabled by adompting an Open Source framework. Specific to the academic setting,  it have been shown suggest that publications that follow open science practices tend to have increased citations. A modern parable for open source software development comes for the epidemiological modelling of the spread of COVID-19 by Prof Neil Ferguson at Imperial Collage London. Crucial to the justification of national lockdowns to kerb the spread of COVID-19, the model was actual developed 13 years prior using undocumented, closed source C++ code. After six weeks of intense revisions and refactoring, with direct help from Microsoft and GitHub software architects,  is now, arguably, the poster child for open source research software.

% https://github.com/mrc-ide/covid-sim

% https://www.nature.com/articles/nature10836?em_x=22

% https://elifesciences.org/articles/16800

\subsection{GUIs or CLIs}

\begin{itemize}
    \item Since the macintosh there has been a continued shift between GUI and CLI statistical software taking the lead \cite{ValeroMora2012}. 
    \item GUIs tend to be associated a dramatically lower learning curve but larger development and maintenance costs.
    \item Better user experience in a quality GUI than CLI \cite{Staggers2000}, however do GUI developers prioritise ease of use over quality of analysis by ignoring key steps (such as quality control).
    \item GUIs are also limited by the number of ways a user can interact with the screen (interactive operations), drop down boxes quickly become unwieldy if they list every possible action to take on an object. 
    \item CLIs are often quicker to develop (so can include the latest statistical methods), easier to scale to larger work loads, better reproducibility and cheaper to maintain.
    \item Without a unified framework to conduct statistical analysis (how you define models and manipulate data) there cannot be an intuitive structure to create and expand a GUI. \cite{Unwin2012}

\end{itemize}

\section{Software documentation}

\subsection{Why is research software documentation important}

Software is intended to be a tool used to complete tasks efficiently.
It alleviates some of the mundane and repetitive tasks to enable users to focus on the complex and abstract. 
However, the intent is not always matched by the execution as users may become frustrated with design idiosyncrasies or suboptimal implementations.
In fact, researchers often dread the steps in their experimental assays that require the use of software or the analysis stages of their studies.
An opinion quickly becoming untenable as the trend trend towards data rich, high throughput studies has led even the most rudimentary analyses to depend on software.
Indeed, as statistical methodologies are still being developed to explore the vast data landscape the practice of reanalysing old experiments is only going to be more common. 
Although the demands for better computational literacy in undergrad and postgrad education is partially addressing these issues only by demanding software meet users halfway by emphasising transparency and accessibility will the problem full resolved.

In order to support reproducibility and the user experience, research software needs to be rewarded on its usability and documentation as well as its functionality.
In order assess the quality of software documentation it is useful to consider the types of documentation broadly separated by its intended audience and content. 
Previous studies have recognised three categories of documentation: documentation of decisions, why did you chose this problem to solve and why did you chose this particular method to solve it; documentation of product, what is contained within this software implementation and how do users interact with it; and documentation of technicalities, how developers created the software and how maintainers can contribute to it. 
Any software intended to be shared contains some product documentation, but few research software projects outline technical details and fewer still mention any decisions made in development \cite{Geiger2018}.

Funding bodies and journals have acknowledged across the board that research data needs to be FAIR (Findable, Accessible, Interoperable and Reusable). \textbf{evidence on uptake of fair practices} However, few guidelines have been implemented on the software packages and analysis pipelines that create/enrich/analysis these dataset to generate conclusions. Are the same FAIR condition relevant to research software? What does software that is FAIR mean? % https://www.rd-alliance.org/system/files/FAIR4RS_Principles_v0.3_RDA-RFC.pdf.

% https://www.researchsquare.com/article/rs-1239393/v1

Open source documentation as well as the software itself helps combat unconscious knowledge (knowledge that the initial developer had but forgets that not everyone knows!) Unconcious knowledge can break early stage tasks especially when the primary developer leaves and there is too much of a steep learning curve for anyone to take over.  

% https://www.software.ac.uk/resources/publications/better-software-better-research

\subsection{Why is research software documentation poor?}

The existence of multiple categories of documentation leads to a diverse set of reasons why users may find a software’s documentation insufficient.
To quantitatively detect patterns in poor software documentation \cite{Aghajani2019} scowered mailing lists, github issues and stack overflow for issues posted by users pertaining problems in documentaion. 
Common issues were based on factually incorrect statements in the documentation, sections of code/functions without any documentation at all or documentation becoming out of date with the latest package versions.
Other issues discuss the ease at which API documentation could be found and searched at all, exactly what terms meant in specific contexts and not having quality translations of documentation in other languages.
As expected, a complete lack of documentation is the most common issue but on the other extreme is dense, unintelligible documentation that is difficult to maintain and search.

Every developer knows the importance of documentation, but new software continues to be created with poor documentation.
In a 2017 GitHub survey of OSS contributors, 93\% reported that “incomplete or outdated documentation is a pervasive problem” but “60\% of contributors say they rarely or never contribute to documentation” \cite{Geiger2017}.
Software documentation typically is the least credited part of software development with little time or funds allocated to its development.
Documentation writers are first to go in times of economic difficulty. \cite{Forward2002}
However, paradoxically writing software documentation requires the most diverse set of skills and experiences to enable people from different backgrounds and knowledge to jump right in at an appropriate level \cite{Geiger2018}.
Documentation writers also envelope a larger populous than those with technical programming skills as the software developers themselves can be limited in their ability to write accessible documentation by unconscious knowledge.

The skills and interests of software developers may not naturally align with the demands of writing documentation, but perhaps the issues actually lies in the lack of community between developer, documenters and users.
Rich Bowen, Community Manager for the CentOS project,  says "There’s common wisdom in the open source world: Everybody knows that the documentation is awful, that nobody wants to write it, and that this is just the way things are. But the truth is that there are lots of people who want to write the docs. We just make it too hard for them to participate. So they write articles on Stack Overflow, on their blogs, and third-party forums. Although this can be good, it’s also a great way for worst-practice solutions to bloom and gain momentum. Embracing these people and making them part of the official documentation effort for your project has many advantages." https://labs.quansight.org/blog/2020/03/documentation-as-a-way- to-build-community/

\subsection{What are the benefits of good research software documentation?}
\begin{itemize}
    \item Documentation acts as "a resource for learning and a second role: as an advertisement for the software project" and the current health of a project.\cite{Geiger2018}
    \item Documentation supports the formation of a developer and maintainer community. Shifting knowledge acquisition away from poorly moderated forums such as biostars or stackoverflow. Extends accessibility of code by highlighting unconscious knowledge and even languages.
    \item Maybe a quick look at the top downloaded R packages on bioconductor and if that correlates with quality documentation?
    \item The quality of hardware and the underlying software dependencies of a package will also change and update, requiring regular maintenance of code for it to be continued to use. However, documentation and reporting the motivations/solutions to problems is often a contribution that last far longer.
    \item Archived and outdated software packages are still being used for the alignment of sequencing data because they are well documented. Simply saying a piece of code is old/out of date is not enough. Justification of the severe negatives of using outdated code should be highlighted in the documentation.
    \item As software get more and more specialised and refined we need to come up with a better way of accrediting work and encouraging documentation so we know exactly how the dependencies run and have a chance of fixing any future eventual bugs. (xkcd meme about software dependency and maintenance. https://xkcd.com/2347/)
\end{itemize}

\subsection{How do we improve research software documentation?}

\begin{itemize}
    \item Lessons learnt from R: vignettes, roxygen, pkgdown
    \item Documentation framework to ensure the right documentation is included and maintainable.
    \item Diataxis is a popular framework for creating documentation along its two axis of knowledge: theory vs practice and acquisition vs application. They separate software documentation into four rough types: Tutorials, practical and ap- plication knowledge; How-tos, practical and acquisition knowledge; references, theoretical and acquisition knowledge; and explaintions, theoretical and application knowledge. %https://diataxis.fr/
    \item User interviews to evaluate documentation as well as functionality
    \item Use of GitHub (issue add transparency to decisions, template issues/pull request help contributors)
    \item Including/understanding the pedagogy of software documentation
    \item Resources available on the Turing Way, open life science and Mozilla
\end{itemize}

\subsection{RNA-seq Analysis Pipelines}

Sequence reads were checked for call quality, repetition and GC content using FASTQC. Each QuantSeq sample was ran over four sequencing lanes and reads were concatenated together. 5PSeq reads contained UMIs due to the additional PCR amplification step and they were separated using UMItools. Illumina adapters and reads with low call quality were removed with Cutadapt. HiSat-2 read aligner was used to align reads from both assays. Both provide paired end reads but with different strandedness. QuantSeq was postive stranded read1 and 5PSeq contained negative stranded read1. The sacCer3 (R64-2) genome build was used for alignment taken from the Saccharomyces Genome Database with the addition of the plasmid sequence. The quality of the alignment was checked by compared the fraction of unambiguously aligned pairs of reads using MultiQC. The gtf was custom made to include the 3'UTRs as detected by pelchena et al. Once reads were aligned the 5PSeq pipeline used UMItools to detect duplicates. The reads mapped to each 3'UTR were counted using featureCounts. Samtools and Bedtools were used throughout to manipulate the alignment files. The analysis of 5PSeq and QuantSeq was conducted using the same NextFlow pipeline was different user defined arguments. A custom script function extracted reads mapped to the 3'UTRs of the constructs of interest.

\section{Ideas}

Bioinformatics Approaches to Gain Insights into cis-Regulatory Motifs Involved in mRNA Localization %https://link.springer.com/chapter/10.1007/978-3-030-31434-7_7

Localization elements and zip codes in the intracellular transport and localization of messenger RNAs in Saccharomyces cerevisiae
%https://wires.onlinelibrary.wiley.com/doi/10.1002/wrna.1591

% https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2015EA000136 https://f1000research.c 295/v1 https://www.exascaleproject.org/event/softwaredocumentation/ https://elib.dlr.de/147388/
% https://headrush.typepad.com/creatingpassionateusers/2006/01/crashcour 
% https://www.proquest.com/docview/304387282
% https://ieeexplore.ieee.org/document/1241364 https://www.jstor.org/stable/44424301
% https://scholar.lib.vt.edu/ejournals/JOTS/Summer- Fall-2000/holmes.html



\end{document}
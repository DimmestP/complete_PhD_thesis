\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../figures/}}}

\begin{document}

\chapter{{Introduction}}
\onehalfspacing

\section{Overview}

The acquisition of quality, high-throughput data sets is outpacing the creation and implementation of methods to handle and analyse it. 
In 2012, the European Bioinformatics Institute (EBI) was one of the biggest biology repositories in the world with a 20Pb storage facility \parencite{EBI2012}. 
However, by 2021, the upload of new data reached 20Pb a year with the institute having to explore collaborations with Google and Amazon in order to keep up \parencite{EBI2021}. 
The “data deluge” \parencite{Royal2012} is not only affecting infrastructure but is leading to fundamental questions about the philosophy of science as scientific publications are starting to benefit from the quality and quantity of their data rather than the significance of the conclusions they reach \parencite{Leonelli2019, Botstein2010}. 
However, the pursuit of big data in biology is undoubtedly leading to answers to questions thought unassailable a few years ago, such as AlphaFold angstrom resolution protein structures for entire genomes \parencite{Jumper2021}. 

Significant progress has been made in developing methods that extract useful information from the “data deluge”.
In investigations of linear covariates, robustness to noise and outliers can be improved by using alternative loss functions, such as the least absolute deviation, or the introduction of penalising terms, such as the $L_n$-norm of the regression coefficients \parencite{Wu2015}.
Reducing the dimensionality of data sets to emphasise regions of interest has also become standard through methods such as principle component analysis \parencite{Wall2005}.  
Methods to compare alternative models of high-dimensional data have been explored including error estimation with cross-validation \parencite{Molinaro2005} or calculating the Akaike information criterion (AIC) \parencite{Akaike1998}.
Finally, conservative methods to test the significance of multiple features whilst maintaining power are widely implemented, e.g. the false discovery rate (FDR) \parencite{Benjamini1995}.

An alternative approach to analysing high-dimensional data sets is the application of flexible deep-learning algorithms.
A variety of ML architectures have been successfully applied to big data across biology ranging from detecting cancer to predicting gene expression \parencite{Xie2017, Liang2015, Tang2019}.
However, complex models appear to be less tolerant of experimental noise and more susceptible to confounding batch effects than simpler models \parencite{Cortes2015, Kaiser2019, Schmitt2021}.
The decisions made by deep-learning algorithms also tend to be difficult to interpret limiting their contribution to improving our understanding \parencite{Caruana2015, Ribeiro2016}.
Other probabilistic methods, such as Bayesian hierarchical models, can offer a balance between interpretability and flexibility, which will be discussed later.
However, whatever the algorithm its effectiveness will be decided by the implementation by the end user. 
Unfortunately, big data scales up the issues inherent to modern biological research alongside novel solutions. 

Science is suffering from a massive reproducibility crisis. 
In 2017, Nature surveyed over 1500 scientists and found over 70\% of them tried and failed to reproduce someone else's work \parencite{Baker2016}.
The origins of the crisis come from the lack of detail in experimental protocols, obscure analysis methods, and misunderstanding of statistical tests.
The level at which scientists mis-used statistical tests has even led a journal to ban any reference to statistical significance \parencite{Trafimow2015}.
Fundamentally, the work by computational biologists is just as susceptible to the reproducibility crisis \parencite{Garijo2013}.
How data is preprocessed, how missing values are dealt with and which software is used can all drastically change results \parencite{Ioannidis2009}.
In big data, as the volume and types of data increases, the sparsity of biologically relevant data points increases which only decrease the likelihood of an analysis being reproducible \parencite{Chattopadhyay2019}.

There remains a gap between the computational methods used to analysis high-dimensional data and the implementation of these methods on biological data.
Those that bridge this gap are typically biologist with no formal software engineering training \parencite{Attwood2019} and who are unlikely to develop prototype analysis scripts into fully fledged programs \parencite{Prins2015}. 
In addition, selection of appropriate models are dependent on subject, hypothesis and quality of data \parencite{Ching2018}.
This leads to high duplication, poor reproducibility and slower overall progress \parencite{Lawlor2015}.
Only through the use of rigorous statistical methods implemented in reproducible and usable software packages can the dawn of the big data effectively combat the reproducibility crisis in biology.

Here, I discuss work using data sets from high-throughput transcriptomics experiments to expand the quantification of regulatory effects on mRNA transcripts. 
Through the development of analysis software and statistical models that incorporate both experimental noise and non-standard interactions I explore transcript localisation and the context dependence of cis-regulatory elements. 
The introduction starts with an overview some of the key mechanisms used by eukaryotic cells to regulate their expression 
I also cover the basics of key transcriptomic assays that have enabled the dawn of the big data in molecular biology. 
qPCR, microarrays and RNA-Seq are covered with emphasis on the sources of error that can be present in these experiments. 
Then, I discuss a handful of statistical methods that enable rigorous analysis of the multi-dimensional data sets with the emphasis of discovering sparse features with biological significance. 
Finally, I outline the key software development practices that have influenced the work I have conducted. 
I implement these best practices to ensure the analysis software I develop is accessible, interoperable and usable.

\section{Thesis layout}

\newpage


\end{document}